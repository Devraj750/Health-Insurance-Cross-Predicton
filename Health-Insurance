
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, log_loss
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb


data = pd.read_csv('data/train.csv')

print("Data Shape:", data.shape)
print("Data Types:\n", data.dtypes)
print("Null Values:\n", data.isnull().sum())

def treat_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df[column] = np.where(df[column] > upper, upper,
                          np.where(df[column] < lower, lower, df[column]))
    return df

data = treat_outliers(data, 'Annual_Premium')
data = treat_outliers(data, 'Vintage')

data['Age_Group'] = pd.cut(data['Age'], bins=[0, 30, 50, 100], labels=['youngAge', 'middleAge', 'oldAge'])

categorical_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Age_Group']
numerical_features = ['Annual_Premium', 'Vintage']

data = pd.get_dummies(data, columns=categorical_features, drop_first=True)

scaler = MinMaxScaler()
data[numerical_features] = scaler.fit_transform(data[numerical_features])

X = data.drop(['id', 'Response'], axis=1)
y = data['Response']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Gaussian NB': GaussianNB(),
    'AdaBoost': AdaBoostClassifier(random_state=42),
    'Bagging': BaggingClassifier(random_state=42),
    'LightGBM': lgb.LGBMClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"\nModel: {name}")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print("ROC AUC Score:", roc_auc_score(y_test, y_pred))
    print("Log Loss:", log_loss(y_test, model.predict_proba(X_test)))

param_grid = {
    'num_leaves': [31, 50],
    'learning_rate': [0.1, 0.01],
    'n_estimators': [100, 200]
}

grid_search = GridSearchCV(lgb.LGBMClassifier(random_state=42), param_grid, cv=3, scoring='roc_auc')
grid_search.fit(X_train, y_train)

print("\nBest Parameters for LightGBM:", grid_search.best_params_)

# Evaluation after Tuning
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
print("\nLightGBM After Tuning")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print("ROC AUC Score:", roc_auc_score(y_test, y_pred))
print("Log Loss:", log_loss(y_test, best_model.predict_proba(X_test)))
